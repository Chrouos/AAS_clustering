{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "from textwrap import dedent\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 讀取 .env 檔案\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_options = {\n",
    "    \"crawler_GDPR\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spider all GDPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page_content(url):\n",
    "    response = requests.get(url)\n",
    "    return BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "def extract_links_from_table(soup, table_id):\n",
    "    table = soup.find('table', {'id': table_id})\n",
    "    if table:\n",
    "        return table.find_all('a')\n",
    "    return []\n",
    "\n",
    "def extract_links_from_div(soup, div_class):\n",
    "    div = soup.find('div', {'class': div_class})\n",
    "    if div:\n",
    "        return div.find_all('a')\n",
    "    return []\n",
    "\n",
    "def extract_text_from_spans(soup, span_class):\n",
    "    spans = soup.find_all('span', {'class': span_class})\n",
    "    return [span.text for span in spans][0]\n",
    "\n",
    "def extract_list_items_from_ol(soup, div_class):\n",
    "    div = soup.find('div', {'class': div_class})\n",
    "    if div:\n",
    "        ols = div.find_all('ol')\n",
    "        return [li.text for ol in ols for li in ol.find_all('li')]\n",
    "    return []\n",
    "\n",
    "def extract_text_from_paragraphs(soup, div_class):\n",
    "    div = soup.find('div', {'class': div_class})\n",
    "    if div:\n",
    "        paragraphs = div.find_all('p')\n",
    "        return [p.text for p in paragraphs]\n",
    "    return []\n",
    "\n",
    "def prepare_data(links, recitals_links):\n",
    "    data = []\n",
    "    for link in links + recitals_links:\n",
    "        text = link.text.strip()\n",
    "        href = link['href']\n",
    "        if \"chapter\" not in text.lower() and 'recitals' not in text.lower():\n",
    "            current_temp = {'Number': text, 'Link': href}\n",
    "            if 'art' in href:\n",
    "                current_temp['Type'] = 'Article'\n",
    "            elif 'recitals' in href:\n",
    "                current_temp['Type'] = 'Recital'\n",
    "            data.append(current_temp)\n",
    "    return data\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawler URL and Article and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdpr_url = \"https://gdpr-info.eu/\"\n",
    "recitals_url = \"https://gdpr-info.eu/recitals/\"\n",
    "    \n",
    "def crawler_url():\n",
    "    gdpr_soup = fetch_page_content(gdpr_url)\n",
    "    recitals_soup = fetch_page_content(recitals_url)\n",
    "\n",
    "    gdpr_links = extract_links_from_table(gdpr_soup, 'tablepress-12')\n",
    "    recitals_links = extract_links_from_div(recitals_soup, 'widget-area recital-widget-area')\n",
    "\n",
    "    print(\"gdpr count:\", len(gdpr_links))\n",
    "    print(\"recitals count:\", len(recitals_links))\n",
    "\n",
    "    data = prepare_data(gdpr_links, recitals_links)\n",
    "    link_df = pd.DataFrame(data)\n",
    "\n",
    "    print(\"GDPR Articles and Recitals Links saved to GDPR_Articles_Recitals_Links.xlsx\")\n",
    "    print(f\"應有 99(GDPR) + 173(Recitals) = {99+173}, 實際有：{len(link_df)}\")\n",
    "\n",
    "    link_df.head()\n",
    "    \n",
    "    return link_df\n",
    "\n",
    "def crawler_article(link_df):\n",
    "    articles_and_recitals_list = []\n",
    "    for index, row in link_df.iterrows():\n",
    "        current_url = row['Link']\n",
    "        current_gdpr_article_content = fetch_page_content(current_url)\n",
    "        title = extract_text_from_spans(current_gdpr_article_content, span_class='dsgvo-title')\n",
    "        content_item_list = extract_list_items_from_ol(current_gdpr_article_content, div_class='entry-content')\n",
    "        content_paragraphs = extract_text_from_paragraphs(current_gdpr_article_content, div_class='entry-content')\n",
    "        \n",
    "        articles_and_recitals_list.append({\n",
    "            'Number': row['Number'],\n",
    "            'Link': row['Link'],\n",
    "            'Type': row['Type'],\n",
    "            'Title': title,\n",
    "            'Content_Items': content_item_list if content_item_list else content_paragraphs\n",
    "        })\n",
    "\n",
    "    articles_df = pd.DataFrame(articles_and_recitals_list)\n",
    "    return articles_df\n",
    "\n",
    "    \n",
    "if working_options[\"crawler_GDPR\"]:\n",
    "    \n",
    "    # - Crawler GDPR Articles and Recitals Links\n",
    "    link_df = crawler_url()\n",
    "    \n",
    "    # - Crawler GDPR Articles\n",
    "    articles_df = crawler_article()\n",
    "    \n",
    "    # - Save to Excel\n",
    "    with pd.ExcelWriter(\"GDPR_Articles_Recitals_Links.xlsx\", engine='xlsxwriter') as writer:\n",
    "        articles_df.to_excel(writer, index=False, sheet_name='Article & Recitals')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有數量: 1705\n",
      "clean_article numbers: 420\n"
     ]
    }
   ],
   "source": [
    "source_data_folder_name = 'batch0716'\n",
    "files = glob.glob(f'{source_data_folder_name}/*/report_*.json')\n",
    "\n",
    "# 提取重複使用的正則表達式\n",
    "article_pattern = r'\\b(Article|Articles|Art|art|article|articles|GDPR)\\b'\n",
    "recital_pattern = r'\\b(recital|Recital|Rec|rec)\\b'\n",
    "\n",
    "# 定義一個函數來讀取和處理單個 JSON 文件\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # 處理 compliant 和 non_compliant 資料\n",
    "    combined_df = pd.concat([\n",
    "        create_dataframe(data.get('compliant', []), 'compliant'),\n",
    "        create_dataframe(data.get('non_compliant', []), 'non-compliant')],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    # 增加一個欄位標示資料夾名稱\n",
    "    folder_name = os.path.basename(os.path.dirname(file_path))\n",
    "    combined_df['folder'] = folder_name\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# 定義一個函數來創建資料框並拆分條文\n",
    "def create_dataframe(data_list, compliance_status):\n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    if 'article numbers' not in df.columns:\n",
    "        df['article numbers'] = ''\n",
    "    \n",
    "    split_df = split_articles(df)\n",
    "    split_df['compliance_status'] = compliance_status\n",
    "    \n",
    "    return split_df\n",
    "\n",
    "# 定義一個函數來拆分條文並新增 Type 欄位\n",
    "def split_articles(df):\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        article_numbers = str(row['article numbers']) if pd.notnull(row['article numbers']) else ''\n",
    "        articles = article_numbers.split(', ')\n",
    "        for article in articles:\n",
    "            new_row = row.copy()\n",
    "            new_row['article numbers'] = article\n",
    "            \n",
    "            # 判斷是 Article 還是 Recital\n",
    "            new_row['type'] = classify_article_type(article)\n",
    "            \n",
    "            rows.append(new_row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# 定義一個函數來分類 Article 或 Recital\n",
    "def classify_article_type(article):\n",
    "    if re.search(article_pattern, article, re.IGNORECASE):\n",
    "        return 'Article'\n",
    "    elif re.search(recital_pattern, article, re.IGNORECASE):\n",
    "        return 'Recital'\n",
    "    else:\n",
    "        return 'Article' # 預設為 Article   \n",
    "\n",
    "# 定義清理文章號碼的函數，只保留整數部分\n",
    "def clean_article_number(article):\n",
    "    match = re.match(r'^\\d+', article)  # 只匹配整數部分\n",
    "    return match.group(0) if match else \"\"\n",
    "\n",
    "# 處理所有文件並合併結果\n",
    "all_data = pd.DataFrame()\n",
    "for file in files:\n",
    "    combined_df = process_file(file)\n",
    "    all_data = pd.concat([all_data, combined_df], ignore_index=True)\n",
    "\n",
    "# 保留原始的 article numbers 並移除贅詞的版本\n",
    "all_data['clean_article numbers'] = (\n",
    "    all_data['article numbers']\n",
    "    .str.replace(article_pattern, '', regex=True)  # 移除 Article 相關的贅詞\n",
    "    .str.replace(recital_pattern, '', regex=True)  # 移除 Recital 相關的贅詞\n",
    "    .str.replace(r'(?<!\\d)\\.(?!\\d)', '', regex=True)  # 移除不在數字之間的點\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# 將 clean_article numbers 轉換成 single_article\n",
    "all_data['single_article'] = all_data['clean_article numbers'].apply(clean_article_number)\n",
    "all_data = all_data[all_data['single_article'].notna() & (all_data['single_article'] != '')]\n",
    "all_data['single_article'] = all_data['single_article'].astype(int)\n",
    "all_data = all_data.sort_values(by='single_article')\n",
    "\n",
    "# 打印數量並顯示頭部數據\n",
    "print(\"所有數量:\", len(all_data))\n",
    "print(\"clean_article numbers:\", len(all_data['article numbers'].unique()))\n",
    "\n",
    "# 將合併後的 DataFrame 存成 Excel 文件\n",
    "excel_file_path = f'./{source_data_folder_name}_cleaned_compliance_data_with_original.xlsx'\n",
    "# all_data.to_excel(excel_file_path, index=False)\n",
    "# all_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the GDPR truth article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>article numbers</th>\n",
       "      <th>legal provisions</th>\n",
       "      <th>type</th>\n",
       "      <th>compliance_status</th>\n",
       "      <th>amend</th>\n",
       "      <th>folder</th>\n",
       "      <th>article_numbers</th>\n",
       "      <th>legal_provisions</th>\n",
       "      <th>article provisions</th>\n",
       "      <th>clean_article numbers</th>\n",
       "      <th>single_article</th>\n",
       "      <th>Content_Items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>This Policy does not apply to the following in...</td>\n",
       "      <td>2(2)(a)</td>\n",
       "      <td>Exemption for controllers or processors in the...</td>\n",
       "      <td>Article</td>\n",
       "      <td>non-compliant</td>\n",
       "      <td>While GDPR allows for exemptions for employee ...</td>\n",
       "      <td>midjourney_old</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2(2)(a)</td>\n",
       "      <td>2</td>\n",
       "      <td>['This Regulation applies to the processing of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>This Privacy Statement does not apply to the i...</td>\n",
       "      <td>2</td>\n",
       "      <td>Material Scope</td>\n",
       "      <td>Article</td>\n",
       "      <td>non-compliant</td>\n",
       "      <td>While separate policies for employees and job ...</td>\n",
       "      <td>zapier_chunks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>['This Regulation applies to the processing of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>位在美國的使用者：請參閱此處，瞭解 Uber 根據美國州政府隱私權法規 (包括《加州消費者隱...</td>\n",
       "      <td>3</td>\n",
       "      <td>While mentioning state laws is not inherently ...</td>\n",
       "      <td>Article</td>\n",
       "      <td>non-compliant</td>\n",
       "      <td>Add a section specifically addressing the appl...</td>\n",
       "      <td>uber_chunks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>['This Regulation applies to the processing of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>This Agreement shall be governed by the laws o...</td>\n",
       "      <td>Article 3</td>\n",
       "      <td>This section might raise concerns under GDPR A...</td>\n",
       "      <td>Article</td>\n",
       "      <td>non-compliant</td>\n",
       "      <td>To ensure GDPR compliance, consider adding a p...</td>\n",
       "      <td>ifttt_chunks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>['This Regulation applies to the processing of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>二、本政策之適用範圍</td>\n",
       "      <td>3</td>\n",
       "      <td>Material Scope</td>\n",
       "      <td>Article</td>\n",
       "      <td>compliant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linetaxi_chunks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>['This Regulation applies to the processing of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                section article numbers  \\\n",
       "1731  This Policy does not apply to the following in...         2(2)(a)   \n",
       "942   This Privacy Statement does not apply to the i...               2   \n",
       "1435  位在美國的使用者：請參閱此處，瞭解 Uber 根據美國州政府隱私權法規 (包括《加州消費者隱...               3   \n",
       "658   This Agreement shall be governed by the laws o...       Article 3   \n",
       "101                                          二、本政策之適用範圍               3   \n",
       "\n",
       "                                       legal provisions     type  \\\n",
       "1731  Exemption for controllers or processors in the...  Article   \n",
       "942                                      Material Scope  Article   \n",
       "1435  While mentioning state laws is not inherently ...  Article   \n",
       "658   This section might raise concerns under GDPR A...  Article   \n",
       "101                                      Material Scope  Article   \n",
       "\n",
       "     compliance_status                                              amend  \\\n",
       "1731     non-compliant  While GDPR allows for exemptions for employee ...   \n",
       "942      non-compliant  While separate policies for employees and job ...   \n",
       "1435     non-compliant  Add a section specifically addressing the appl...   \n",
       "658      non-compliant  To ensure GDPR compliance, consider adding a p...   \n",
       "101          compliant                                                NaN   \n",
       "\n",
       "               folder article_numbers legal_provisions article provisions  \\\n",
       "1731   midjourney_old             NaN              NaN                NaN   \n",
       "942     zapier_chunks             NaN              NaN                NaN   \n",
       "1435      uber_chunks             NaN              NaN                NaN   \n",
       "658      ifttt_chunks             NaN              NaN                NaN   \n",
       "101   linetaxi_chunks             NaN              NaN                NaN   \n",
       "\n",
       "     clean_article numbers  single_article  \\\n",
       "1731               2(2)(a)               2   \n",
       "942                      2               2   \n",
       "1435                     3               3   \n",
       "658                      3               3   \n",
       "101                      3               3   \n",
       "\n",
       "                                          Content_Items  \n",
       "1731  ['This Regulation applies to the processing of...  \n",
       "942   ['This Regulation applies to the processing of...  \n",
       "1435  ['This Regulation applies to the processing of...  \n",
       "658   ['This Regulation applies to the processing of...  \n",
       "101   ['This Regulation applies to the processing of...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdpr_articles_recitals_links_df = pd.read_excel(\"./GDPR_Articles_Recitals_Links.xlsx\")\n",
    "\n",
    "for index, row in all_data.iterrows():\n",
    "    # 找到相同的 single_article 和 type 的行\n",
    "    matching_rows = gdpr_articles_recitals_links_df[\n",
    "        (gdpr_articles_recitals_links_df['Number'] == row[\"single_article\"]) &\n",
    "        (gdpr_articles_recitals_links_df['Type'] == row[\"type\"])\n",
    "    ]\n",
    "    \n",
    "    # 如果有匹配的行，將 Content_Items 的值加入到 all_data 中\n",
    "    if not matching_rows.empty:\n",
    "        # 將匹配行的 Content_Items 內容拼接成一個字符串\n",
    "        content_items = \"; \".join(matching_rows['Content_Items'].astype(str))\n",
    "        all_data.at[index, 'Content_Items'] = content_items\n",
    "        \n",
    "# all_data.to_excel(excel_file_path, index=False)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully set GPT_KEY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1705it [21:58,  1.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>article numbers</th>\n",
       "      <th>legal provisions</th>\n",
       "      <th>type</th>\n",
       "      <th>compliance_status</th>\n",
       "      <th>amend</th>\n",
       "      <th>folder</th>\n",
       "      <th>article_numbers</th>\n",
       "      <th>legal_provisions</th>\n",
       "      <th>article provisions</th>\n",
       "      <th>clean_article numbers</th>\n",
       "      <th>single_article</th>\n",
       "      <th>Content_Items</th>\n",
       "      <th>is_correct_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>This Policy does not apply to the following in...</td>\n",
       "      <td>2(2)(a)</td>\n",
       "      <td>Exemption for controllers or processors in the...</td>\n",
       "      <td>Article</td>\n",
       "      <td>non-compliant</td>\n",
       "      <td>While GDPR allows for exemptions for employee ...</td>\n",
       "      <td>midjourney_old</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2(2)(a)</td>\n",
       "      <td>2</td>\n",
       "      <td>['This Regulation applies to the processing of...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>This Privacy Statement does not apply to the i...</td>\n",
       "      <td>2</td>\n",
       "      <td>Material Scope</td>\n",
       "      <td>Article</td>\n",
       "      <td>non-compliant</td>\n",
       "      <td>While separate policies for employees and job ...</td>\n",
       "      <td>zapier_chunks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>['This Regulation applies to the processing of...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>位在美國的使用者：請參閱此處，瞭解 Uber 根據美國州政府隱私權法規 (包括《加州消費者隱...</td>\n",
       "      <td>3</td>\n",
       "      <td>While mentioning state laws is not inherently ...</td>\n",
       "      <td>Article</td>\n",
       "      <td>non-compliant</td>\n",
       "      <td>Add a section specifically addressing the appl...</td>\n",
       "      <td>uber_chunks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>['This Regulation applies to the processing of...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>This Agreement shall be governed by the laws o...</td>\n",
       "      <td>Article 3</td>\n",
       "      <td>This section might raise concerns under GDPR A...</td>\n",
       "      <td>Article</td>\n",
       "      <td>non-compliant</td>\n",
       "      <td>To ensure GDPR compliance, consider adding a p...</td>\n",
       "      <td>ifttt_chunks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>['This Regulation applies to the processing of...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>二、本政策之適用範圍</td>\n",
       "      <td>3</td>\n",
       "      <td>Material Scope</td>\n",
       "      <td>Article</td>\n",
       "      <td>compliant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>linetaxi_chunks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>['This Regulation applies to the processing of...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                section article numbers  \\\n",
       "1731  This Policy does not apply to the following in...         2(2)(a)   \n",
       "942   This Privacy Statement does not apply to the i...               2   \n",
       "1435  位在美國的使用者：請參閱此處，瞭解 Uber 根據美國州政府隱私權法規 (包括《加州消費者隱...               3   \n",
       "658   This Agreement shall be governed by the laws o...       Article 3   \n",
       "101                                          二、本政策之適用範圍               3   \n",
       "\n",
       "                                       legal provisions     type  \\\n",
       "1731  Exemption for controllers or processors in the...  Article   \n",
       "942                                      Material Scope  Article   \n",
       "1435  While mentioning state laws is not inherently ...  Article   \n",
       "658   This section might raise concerns under GDPR A...  Article   \n",
       "101                                      Material Scope  Article   \n",
       "\n",
       "     compliance_status                                              amend  \\\n",
       "1731     non-compliant  While GDPR allows for exemptions for employee ...   \n",
       "942      non-compliant  While separate policies for employees and job ...   \n",
       "1435     non-compliant  Add a section specifically addressing the appl...   \n",
       "658      non-compliant  To ensure GDPR compliance, consider adding a p...   \n",
       "101          compliant                                                NaN   \n",
       "\n",
       "               folder article_numbers legal_provisions article provisions  \\\n",
       "1731   midjourney_old             NaN              NaN                NaN   \n",
       "942     zapier_chunks             NaN              NaN                NaN   \n",
       "1435      uber_chunks             NaN              NaN                NaN   \n",
       "658      ifttt_chunks             NaN              NaN                NaN   \n",
       "101   linetaxi_chunks             NaN              NaN                NaN   \n",
       "\n",
       "     clean_article numbers  single_article  \\\n",
       "1731               2(2)(a)               2   \n",
       "942                      2               2   \n",
       "1435                     3               3   \n",
       "658                      3               3   \n",
       "101                      3               3   \n",
       "\n",
       "                                          Content_Items is_correct_en  \n",
       "1731  ['This Regulation applies to the processing of...         False  \n",
       "942   ['This Regulation applies to the processing of...         False  \n",
       "1435  ['This Regulation applies to the processing of...         False  \n",
       "658   ['This Regulation applies to the processing of...         False  \n",
       "101   ['This Regulation applies to the processing of...         False  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取得 GPT-KEY\n",
    "gpt_key = os.getenv('GPT_KEY')\n",
    "if gpt_key is not None: print(\"Successfully set GPT_KEY\")\n",
    "\n",
    "def validate_prompt(original_section, original_article_number):\n",
    "    return dedent(f'''\n",
    "    Section=\n",
    "    ```\n",
    "    {original_section}\n",
    "    ```\n",
    "    是否符合 GDPR 第 {original_article_number} 條？ 只要相關就可以\n",
    "    \n",
    "    ```\n",
    "    OUTPUT=\n",
    "    {{\n",
    "    \"is_correct\": true/false\n",
    "    }}\n",
    "    ```\n",
    "    返回結果為一行JSON格式字串，無換行或特殊符號\n",
    "    ''')\n",
    "\n",
    "all_data['is_correct'] = None\n",
    "prompt_list = []\n",
    "\n",
    "for index, row in tqdm(all_data.iterrows()):\n",
    "    \n",
    "    count = 0\n",
    "    try:\n",
    "        prompt_content = validate_prompt(row['section'], row['single_article'])\n",
    "        prompt_list.append(prompt_content)\n",
    "\n",
    "        client = OpenAI(api_key=gpt_key)\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a GDPR checker.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_content}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        for repeat_index in range(3):\n",
    "            try:\n",
    "                # 取得回覆\n",
    "                response = completion.choices[0].message.content.strip()\n",
    "                response_dict = json.loads(response)\n",
    "\n",
    "                # 更新 DataFrame\n",
    "                all_data.at[index, 'is_correct'] = False\n",
    "                response_correct = str(response_dict.get('is_correct')) \n",
    "                if response_correct == 'True' : \n",
    "                    count += 1\n",
    "                    if count > 1:\n",
    "                        all_data.at[index, 'is_correct'] = str(response_dict.get('is_correct')) \n",
    "                        break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        all_data.at[index, 'is_correct'] = 'Error'\n",
    "\n",
    "separator = '-' * 40\n",
    "with open('output.txt', 'w', encoding='utf-8') as file:\n",
    "    for prompt_content in prompt_list:\n",
    "        file.write(prompt_content + '\\n')\n",
    "        file.write(separator + '\\n')\n",
    "\n",
    "# 保存 DataFrame 到 Excel\n",
    "all_data.to_excel(excel_file_path, index=False)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'is_correct'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'is_correct'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m all_data[\u001b[38;5;241m~\u001b[39m\u001b[43mall_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mis_correct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNONE\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m&\u001b[39m (all_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_correct_en\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(filtered_data\u001b[38;5;241m.\u001b[39mhead()\u001b[38;5;241m.\u001b[39mto_dict())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m總數：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'is_correct'"
     ]
    }
   ],
   "source": [
    "filtered_data = all_data[~all_data['is_correct'].isin(['ERROR', 'NONE']) & (all_data['is_correct_en'] != False)]\n",
    "print(filtered_data.head().to_dict())\n",
    "print(f\"總數：{len(filtered_data)}\")\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "總數：49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kc/jbnf20ls6hl2qsfkbcc8pb600000gn/T/ipykernel_85513/2975367219.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined_df = filtered_data.groupby(['type', 'single_article', 'Content_Items']).apply(combine_rows).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    {'type': 'Article', 'single_article': 2, 'Cont...\n",
       "1    {'type': 'Article', 'single_article': 3, 'Cont...\n",
       "2    {'type': 'Article', 'single_article': 4, 'Cont...\n",
       "3    {'type': 'Article', 'single_article': 5, 'Cont...\n",
       "4    {'type': 'Article', 'single_article': 6, 'Cont...\n",
       "dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 定義要輸出的 JSON 檔路徑\n",
    "output_json_path = f\"{source_data_folder_name}_combine_article_filter.json\"\n",
    "\n",
    "def combine_rows(group):\n",
    "    return {\n",
    "        \"type\": group.name[0],\n",
    "        \"single_article\": group.name[1],\n",
    "        \"Content_Items\": list(set(group['Content_Items'])),  # 使用 set 去重並轉換回列表\n",
    "        \"compliant\": group[group['compliance_status'] == 'compliant'][['folder', 'section', 'legal provisions', 'amend']].to_dict(orient='records'),\n",
    "        \"non-compliant\": group[group['compliance_status'] == 'non-compliant'][['folder', 'section', 'legal provisions', 'amend']].to_dict(orient='records')\n",
    "    }\n",
    "\n",
    "# 根據 type、single_article 和 Content_Items 分組並合併\n",
    "combined_df = filtered_data.groupby(['type', 'single_article', 'Content_Items']).apply(combine_rows).reset_index(drop=True)\n",
    "\n",
    "# 將結果轉換為 JSON 並輸出至檔案\n",
    "combined_df.to_json(output_json_path, orient='records', force_ascii=False)\n",
    "\n",
    "print(f\"總數：{len(combined_df)}\")\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "# import numpy as np\n",
    "# import plotly.express as px   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file_path = f'{source_data_folder_name}_cluster_compliance_data.xlsx' \n",
    "# cluster_writer_excel = pd.ExcelWriter(output_file_path, engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 讀取上傳的 Excel 文件\n",
    "# file_path = excel_file_path\n",
    "# compliance_data_df = pd.read_excel(file_path)\n",
    "\n",
    "# # 將各欄位分別進行 TF-IDF 向量化\n",
    "# def vectorize_column(data, column_name):\n",
    "#     vectorizer = TfidfVectorizer(stop_words='english')\n",
    "#     vectors = vectorizer.fit_transform(data[column_name].astype(str).tolist())\n",
    "#     return vectors\n",
    "\n",
    "# vector_section = vectorize_column(compliance_data_df, 'section')\n",
    "# vector_article_numbers = vectorize_column(compliance_data_df, 'clean_article numbers')\n",
    "# vector_legal_provisions = vectorize_column(compliance_data_df, 'legal provisions')\n",
    "\n",
    "# # 合併各欄位的向量表示\n",
    "# from scipy.sparse import hstack\n",
    "# X_combined = hstack([vector_section, vector_article_numbers, vector_legal_provisions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "手肘法（Elbow Method）：  \n",
    "通過計算不同k值下的SSE（Sum of Squared Errors）來找到最佳k值，SSE會隨著k的增加而減小，當SSE減小幅度變緩時，即所謂的“手肘”點，即為最佳k值。\n",
    "\n",
    "輪廓係數（Silhouette Coefficient）：  \n",
    "輪廓係數能同時考慮簇內和簇間距離，其值在-1到1之間，越接近1說明聚類效果越好。可以計算不同k值下的平均輪廓係數來選擇最佳k值。\n",
    "\n",
    "Calinski-Harabasz Index：  \n",
    "該指標基於簇內和簇間距離計算，值越大越好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定義範圍\n",
    "# max_k = 5  # 限制最大k值\n",
    "# k_range = range(2, max_k)\n",
    "\n",
    "# # 保存不同指標的結果\n",
    "# sse = []\n",
    "# silhouette_scores = []\n",
    "# calinski_scores = []\n",
    "# davies_scores = []\n",
    "\n",
    "# for k in k_range:\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "#     clusters = kmeans.fit_predict(X_combined)\n",
    "#     sse.append(kmeans.inertia_)\n",
    "#     silhouette_scores.append(silhouette_score(X_combined, clusters))\n",
    "#     calinski_scores.append(calinski_harabasz_score(X_combined.toarray(), clusters))\n",
    "#     davies_scores.append(davies_bouldin_score(X_combined.toarray(), clusters))\n",
    "\n",
    "# # 找到手肘點 (SSE)\n",
    "# diff = np.diff(sse)\n",
    "# diff_r = diff[1:] / diff[:-1]\n",
    "# knee_point = np.argmin(diff_r) + 2\n",
    "\n",
    "# # 找到最大輪廓係數點\n",
    "# best_silhouette = np.argmax(silhouette_scores) + 2\n",
    "\n",
    "# # 找到最大Calinski-Harabasz Index點\n",
    "# best_calinski = np.argmax(calinski_scores) + 2\n",
    "\n",
    "# # 找到最小Davies-Bouldin Index點\n",
    "# best_davies = np.argmin(davies_scores) + 2\n",
    "\n",
    "# # 綜合考慮這些指標，選擇最常出現的分群數量\n",
    "# best_k_candidates = [knee_point, best_silhouette, best_calinski, best_davies]\n",
    "# best_k = max(set(best_k_candidates), key=best_k_candidates.count)\n",
    "\n",
    "# print(f\"手肘法最佳分群數量: {knee_point}\")\n",
    "# print(f\"最大輪廓係數最佳分群數量: {best_silhouette}\")\n",
    "# print(f\"最大Calinski-Harabasz Index最佳分群數量: {best_calinski}\")\n",
    "# print(f\"最小Davies-Bouldin Index最佳分群數量: {best_davies}\")\n",
    "# print(f\"綜合考慮的最佳分群數量: {best_k}\")\n",
    "\n",
    "# # 使用K-means進行文本分群\n",
    "# kmeans = KMeans(n_clusters=best_k, random_state=42, n_init='auto')  \n",
    "# clusters = kmeans.fit_predict(X_combined)\n",
    "\n",
    "# # 將分群結果添加回數據框\n",
    "# compliance_data_df['cluster'] = clusters\n",
    "\n",
    "# # 使用PCA進行降維\n",
    "# pca = PCA(n_components=2)\n",
    "# X_pca_2d = pca.fit_transform(X_combined.toarray())\n",
    "\n",
    "# # 將降維結果添加回數據框\n",
    "# compliance_data_df['pca-2d-one'] = X_pca_2d[:,0]\n",
    "# compliance_data_df['pca-2d-two'] = X_pca_2d[:,1]\n",
    "\n",
    "# # 使用Plotly進行互動式視覺化\n",
    "# fig = px.scatter(\n",
    "#     compliance_data_df, x='pca-2d-one', y='pca-2d-two', color='cluster',\n",
    "#     hover_data=['section', 'clean_article numbers', 'legal provisions', 'compliance_status'],\n",
    "#     title=\"PCA Clustering of Legal Provisions\"\n",
    "# )\n",
    "# fig.show()\n",
    "\n",
    "\n",
    "# compliance_data_df.to_excel(cluster_writer_excel, index=False, sheet_name='K_means')\n",
    "# print(f\"分群結果已保存到 {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# # 定義範圍\n",
    "# max_k = 5  # 限制最大k值\n",
    "# k_range = range(2, max_k)\n",
    "\n",
    "# # 保存不同指標的結果\n",
    "# silhouette_scores = []\n",
    "# calinski_scores = []\n",
    "# davies_scores = []\n",
    "\n",
    "# for k in k_range:\n",
    "#     clustering = AgglomerativeClustering(n_clusters=k)\n",
    "#     clusters = clustering.fit_predict(X_combined.toarray())\n",
    "#     silhouette_scores.append(silhouette_score(X_combined, clusters))\n",
    "#     calinski_scores.append(calinski_harabasz_score(X_combined.toarray(), clusters))\n",
    "#     davies_scores.append(davies_bouldin_score(X_combined.toarray(), clusters))\n",
    "\n",
    "# # 找到最大輪廓係數點\n",
    "# best_silhouette = np.argmax(silhouette_scores) + 2\n",
    "\n",
    "# # 找到最大Calinski-Harabasz Index點\n",
    "# best_calinski = np.argmax(calinski_scores) + 2\n",
    "\n",
    "# # 找到最小Davies-Bouldin Index點\n",
    "# best_davies = np.argmin(davies_scores) + 2\n",
    "\n",
    "# # 綜合考慮這些指標，選擇最常出現的分群數量\n",
    "# best_k_candidates = [best_silhouette, best_calinski, best_davies]\n",
    "# best_k = max(set(best_k_candidates), key=best_k_candidates.count)\n",
    "\n",
    "# print(f\"最大輪廓係數最佳分群數量: {best_silhouette}\")\n",
    "# print(f\"最大Calinski-Harabasz Index最佳分群數量: {best_calinski}\")\n",
    "# print(f\"最小Davies-Bouldin Index最佳分群數量: {best_davies}\")\n",
    "# print(f\"綜合考慮的最佳分群數量: {best_k}\")\n",
    "\n",
    "# # 使用AgglomerativeClustering進行文本分群\n",
    "# clustering = AgglomerativeClustering(n_clusters=best_k)  \n",
    "# clusters = clustering.fit_predict(X_combined.toarray())\n",
    "\n",
    "# # 將分群結果添加回數據框\n",
    "# compliance_data_df['cluster'] = clusters\n",
    "\n",
    "# # 使用PCA進行降維\n",
    "# pca = PCA(n_components=2)\n",
    "# X_pca_2d = pca.fit_transform(X_combined.toarray())\n",
    "\n",
    "# # 將降維結果添加回數據框\n",
    "# compliance_data_df['pca-2d-one'] = X_pca_2d[:,0]\n",
    "# compliance_data_df['pca-2d-two'] = X_pca_2d[:,1]\n",
    "\n",
    "# # 使用Plotly進行互動式視覺化\n",
    "# fig = px.scatter(\n",
    "#     compliance_data_df, x='pca-2d-one', y='pca-2d-two', color='cluster',\n",
    "#     hover_data=['section', 'clean_article numbers', 'legal provisions', 'compliance_status'],\n",
    "#     title=\"PCA Clustering of Legal Provisions\"\n",
    "# )\n",
    "# fig.show()\n",
    "\n",
    "# compliance_data_df.to_excel(cluster_writer_excel, index=False, sheet_name='Agglomerative')\n",
    "# print(f\"分群結果已保存到 {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_writer_excel.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
